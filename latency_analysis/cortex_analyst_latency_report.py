"""
Cortex Analyst Latency Report

A focused Streamlit app for analyzing Cortex Analyst query latency breakdown,
showing both SQL generation time and execution time.
"""

import streamlit as st
import pandas as pd
import datetime
from snowflake.snowpark import Session
from snowflake.snowpark.context import get_active_session


def get_session():
    """Get Snowpark session - works in both local and SiS environments"""
    try:
        session = get_active_session()
        return session
    except:
        import configparser
        config = configparser.ConfigParser()
        config.read("config.env")
        conn_name = "connections.my_example_connection"

        conn_params = {
            "account": config[conn_name]["account"].strip('"'),
            "user": config[conn_name]["user"].strip('"'),
            "role": config[conn_name]["role"].strip('"'),
            "warehouse": config[conn_name]["warehouse"].strip('"'),
            "database": config[conn_name]["database"].strip('"'),
            "schema": config[conn_name]["schema"].strip('"')
        }

        if "password" in config[conn_name]:
            conn_params["password"] = config[conn_name]["password"].strip('"')

        return Session.builder.configs(conn_params).create()


def fetch_semantic_model_paths(session):
    """
    Fetch semantic model paths from all agents in the schema
    """
    try:
        agents_result = session.sql(
            "SHOW AGENTS IN SCHEMA snowflake_intelligence.agents").collect()

        if not agents_result:
            return pd.DataFrame()

        results = []
        agent_names = [row[1] for row in agents_result]

        for agent in agent_names:
            try:
                describe_sql = f'DESCRIBE AGENT SNOWFLAKE_INTELLIGENCE.AGENTS."{agent}"'
                describe_result = session.sql(describe_sql).collect()

                if not describe_result:
                    continue

                agent_spec_json = describe_result[0][6] if len(
                    describe_result[0]) > 6 else None

                if agent_spec_json:
                    import json
                    try:
                        spec = json.loads(agent_spec_json)
                        if 'tool_resources' in spec:
                            for tool_name, tool_data in spec['tool_resources'].items():
                                semantic_file = tool_data.get(
                                    'semantic_model_file')
                                if semantic_file:
                                    results.append({
                                        "agent_name": agent,
                                        "tool_name": tool_name,
                                        "semantic_model_file": semantic_file
                                    })
                    except json.JSONDecodeError:
                        continue
            except Exception:
                continue
    except Exception:
        return pd.DataFrame()

    return pd.DataFrame(results)


def get_cortex_analyst_latency_data(session, semantic_model_file, start_date, end_date):
    """
    Fetch Cortex Analyst requests with latency breakdown for a specific semantic model
    """
    sql = f"""
    SELECT 
        timestamp,
        request_id,
        semantic_model_name,
        user_name,
        response_body:response_metadata.analyst_latency_ms::NUMBER as analyst_latency_ms,
        request_body:messages[0].content[0].text::STRING as user_question,
        generated_sql,
        response_status_code
    FROM TABLE(
        SNOWFLAKE.LOCAL.CORTEX_ANALYST_REQUESTS('FILE_ON_STAGE', '{semantic_model_file}')
    )
    WHERE response_status_code = 200
    AND timestamp BETWEEN '{start_date}'::TIMESTAMP AND '{end_date}'::TIMESTAMP
    ORDER BY timestamp DESC
    """
    return session.sql(sql).to_pandas()


def get_all_cortex_analyst_requests(session, start_date, end_date):
    """
    Fetch all Cortex Analyst requests across all semantic models and views.
    Requires ACCOUNTADMIN or SNOWFLAKE.CORTEX_ANALYST_REQUESTS_ADMIN application role.
    """
    sql = f"""
    SELECT 
        timestamp,
        request_id,
        semantic_model_name,
        user_name,
        response_body:response_metadata.analyst_latency_ms::NUMBER as analyst_latency_ms,
        request_body:messages[0].content[0].text::STRING as user_question,
        generated_sql,
        response_status_code
    FROM SNOWFLAKE.LOCAL.CORTEX_ANALYST_REQUESTS_V
    WHERE response_status_code = 200
    AND timestamp BETWEEN '{start_date}'::TIMESTAMP AND '{end_date}'::TIMESTAMP
    ORDER BY timestamp DESC
    """
    return session.sql(sql).to_pandas()


def get_sql_execution_latency(session, start_date, end_date):
    """
    Fetch SQL execution latency for queries generated by Cortex Analyst
    """
    sql = f"""
    SELECT 
        query_id,
        query_text,
        query_tag,
        start_time,
        end_time,
        total_elapsed_time as execution_time_ms,
        user_name,
        warehouse_name,
        execution_status
    FROM SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY
    WHERE start_time BETWEEN '{start_date}'::TIMESTAMP AND '{end_date}'::TIMESTAMP
    AND query_text LIKE '%Generated by Cortex Analyst%'
    AND execution_status = 'SUCCESS'
    ORDER BY start_time DESC
    """
    return session.sql(sql).to_pandas()


def join_latency_data(analyst_df, query_df):
    """
    Join Cortex Analyst data with query execution data
    """
    import re

    def normalize_sql(sql_text):
        if pd.isna(sql_text) or sql_text is None:
            return ""
        sql_str = str(sql_text).strip()
        sql_str = re.sub(r'--[^\n]*\n', '\n', sql_str)
        sql_str = re.sub(r'/\*.*?\*/', ' ', sql_str)
        sql_normalized = re.sub(r'\s+', ' ', sql_str.upper())
        return sql_normalized.rstrip(';')

    analyst_df = analyst_df.copy()
    query_df = query_df.copy()

    analyst_df['NORMALIZED_SQL'] = analyst_df['GENERATED_SQL'].apply(
        normalize_sql)
    query_df['NORMALIZED_SQL'] = query_df['QUERY_TEXT'].apply(normalize_sql)

    analyst_df = analyst_df[analyst_df['NORMALIZED_SQL'] != ""]
    query_df = query_df[query_df['NORMALIZED_SQL'] != ""]

    merged = pd.merge(
        analyst_df,
        query_df[['NORMALIZED_SQL', 'QUERY_ID',
                  'QUERY_TEXT', 'EXECUTION_TIME_MS', 'WAREHOUSE_NAME']],
        on='NORMALIZED_SQL',
        how='left'
    )

    merged['TOTAL_LATENCY_MS'] = merged['ANALYST_LATENCY_MS'].fillna(
        0) + merged['EXECUTION_TIME_MS'].fillna(0)
    merged['ANALYST_LATENCY_SEC'] = merged['ANALYST_LATENCY_MS'] / 1000
    merged['EXECUTION_LATENCY_SEC'] = merged['EXECUTION_TIME_MS'] / 1000
    merged['TOTAL_LATENCY_SEC'] = merged['TOTAL_LATENCY_MS'] / 1000

    return merged


def generate_html_report(analyst_df, slowest_analyst, query_df=None, slowest_sql=None, model_breakdown=None, query_tag_dist=None, warehouse_dist=None):
    """
    Generate comprehensive HTML report for sharing including all analysis sections
    """
    avg_analyst = analyst_df['ANALYST_LATENCY_SEC'].mean()
    median_analyst = analyst_df['ANALYST_LATENCY_SEC'].median()
    max_analyst = analyst_df['ANALYST_LATENCY_SEC'].max()

    html = f"""
    <!DOCTYPE html>
    <html>
    <head>
        <title>Cortex Analyst Latency Report</title>
        <style>
            body {{ font-family: Arial, sans-serif; margin: 20px; background: #f5f5f5; }}
            .container {{ max-width: 1400px; margin: 0 auto; background: white; padding: 30px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }}
            h1 {{ color: #29B5E8; border-bottom: 3px solid #29B5E8; padding-bottom: 10px; }}
            h2 {{ color: #666; margin-top: 40px; border-bottom: 2px solid #ddd; padding-bottom: 8px; }}
            h3 {{ color: #29B5E8; margin-top: 25px; }}
            table {{ border-collapse: collapse; width: 100%; margin-top: 15px; margin-bottom: 25px; }}
            th {{ background-color: #29B5E8; color: white; padding: 12px; text-align: left; font-weight: 600; }}
            td {{ border: 1px solid #ddd; padding: 10px; }}
            tr:nth-child(even) {{ background-color: #f9f9f9; }}
            tr:hover {{ background-color: #f0f8ff; }}
            .metric {{ display: inline-block; margin: 20px 20px 20px 0; padding: 20px; background: #f0f8ff; border-radius: 8px; min-width: 200px; }}
            .metric-value {{ font-size: 32px; font-weight: bold; color: #29B5E8; }}
            .metric-label {{ font-size: 14px; color: #666; margin-top: 5px; }}
            .timestamp {{ color: #999; font-size: 12px; margin-top: 20px; padding: 10px; background: #f9f9f9; border-radius: 5px; }}
            .section {{ margin: 30px 0; padding: 20px; background: #fafafa; border-left: 4px solid #29B5E8; }}
            .warning {{ background: #fff3cd; border-left: 4px solid #ffc107; padding: 15px; margin: 20px 0; }}
        </style>
    </head>
    <body>
        <div class="container">
        <h1>üèîÔ∏è Cortex Analyst Latency Report</h1>
        <div class="timestamp">Generated: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</div>
        
        <div class="section">
        <h2>üìä Cortex Analyst SQL Generation Analysis</h2>
        <p><strong>Total Requests Analyzed:</strong> {len(analyst_df)}</p>
        
        <h3>Summary Metrics</h3>
        <div class="metric">
            <div class="metric-label">Avg SQL Generation</div>
            <div class="metric-value">{avg_analyst:.2f}s</div>
        </div>
        <div class="metric">
            <div class="metric-label">Median SQL Generation</div>
            <div class="metric-value">{median_analyst:.2f}s</div>
        </div>
        <div class="metric">
            <div class="metric-label">Max SQL Generation</div>
            <div class="metric-value">{max_analyst:.2f}s</div>
        </div>
        <div style="clear: both;"></div>
    """

    if model_breakdown is not None and not model_breakdown.empty:
        html += f"""
        <h3>Breakdown by Semantic Model</h3>
        {model_breakdown.to_html(index=False, classes='data-table')}
        """

    html += f"""
        <h3>Slowest Cortex Analyst Requests</h3>
        {slowest_analyst.to_html(index=False, classes='data-table')}
        </div>
    """

    if query_df is not None and len(query_df) > 0:
        avg_exec = query_df['EXECUTION_LATENCY_SEC'].mean()
        median_exec = query_df['EXECUTION_LATENCY_SEC'].median()
        max_exec = query_df['EXECUTION_LATENCY_SEC'].max()

        html += f"""
        <div class="section">
        <h2>‚ö° SQL Execution Analysis</h2>
        <p><strong>Total SQL Executions Analyzed:</strong> {len(query_df)}</p>
        
        <h3>Summary Metrics</h3>
        <div class="metric">
            <div class="metric-label">Avg SQL Execution</div>
            <div class="metric-value">{avg_exec:.2f}s</div>
        </div>
        <div class="metric">
            <div class="metric-label">Median SQL Execution</div>
            <div class="metric-value">{median_exec:.2f}s</div>
        </div>
        <div class="metric">
            <div class="metric-label">Max SQL Execution</div>
            <div class="metric-value">{max_exec:.2f}s</div>
        </div>
        <div style="clear: both;"></div>
        """

        if query_tag_dist is not None and not query_tag_dist.empty:
            html += f"""
            <h3>Query Tag Distribution</h3>
            {query_tag_dist.to_html(index=False, classes='data-table')}
            """

        if warehouse_dist is not None and not warehouse_dist.empty:
            html += f"""
            <h3>Warehouse Distribution</h3>
            {warehouse_dist.to_html(index=False, classes='data-table')}
            """

        if slowest_sql is not None and not slowest_sql.empty:
            html += f"""
            <h3>Slowest SQL Executions</h3>
            {slowest_sql.to_html(index=False, classes='data-table')}
            """

        html += "</div>"

    html += """
        <div class="warning">
        <strong>‚ö†Ô∏è Note on Combined Analysis:</strong> Joining Cortex Analyst logs with Query History by SQL text matching can be inaccurate. 
        The sections above provide separate, reliable analysis of SQL generation and execution latency.
        </div>
        
        </div>
    </body>
    </html>
    """
    return html


st.title("üèîÔ∏è Cortex Analyst Latency Report")
st.markdown(
    "**Analysis of SQL generation and execution latency for Cortex Analyst queries**")
st.divider()

session = get_session()

st.markdown("### üìÖ Select Analysis Period")
col1, col2, col3, col4 = st.columns(4)

if 'start_date' not in st.session_state:
    st.session_state.start_date = datetime.datetime.now() - datetime.timedelta(days=7)
if 'end_date' not in st.session_state:
    st.session_state.end_date = datetime.datetime.now()

if col1.button('7 Days'):
    st.session_state.start_date = datetime.datetime.now() - datetime.timedelta(days=7)
    st.session_state.end_date = datetime.datetime.now()
if col2.button('30 Days'):
    st.session_state.start_date = datetime.datetime.now() - datetime.timedelta(days=30)
    st.session_state.end_date = datetime.datetime.now()
if col3.button('60 Days'):
    st.session_state.start_date = datetime.datetime.now() - datetime.timedelta(days=60)
    st.session_state.end_date = datetime.datetime.now()
if col4.button('90 Days'):
    st.session_state.start_date = datetime.datetime.now() - datetime.timedelta(days=90)
    st.session_state.end_date = datetime.datetime.now()

date_range = st.date_input(
    "Custom Date Range",
    value=(st.session_state.start_date.date(),
           st.session_state.end_date.date()),
    max_value=datetime.date.today()
)

if len(date_range) == 2:
    st.session_state.start_date = datetime.datetime.combine(
        date_range[0], datetime.time.min)
    st.session_state.end_date = datetime.datetime.combine(
        date_range[1], datetime.time.max)

start_date = st.session_state.start_date
end_date = st.session_state.end_date

st.divider()

st.markdown("### üóÇÔ∏è Data Source Selection")

data_source = st.radio(
    "Select Data Source",
    options=[
        "Specific Semantic Model (from Agent)",
        "All Semantic Models (ACCOUNTADMIN required)"
    ],
    help="Choose to analyze a specific semantic model or all models at once"
)

if data_source == "Specific Semantic Model (from Agent)":
    with st.spinner("Fetching semantic models..."):
        semantic_models_df = fetch_semantic_model_paths(session)

    if semantic_models_df.empty:
        st.error(
            "No semantic models found. Please ensure you have Cortex Agents configured.")
        st.stop()

    semantic_model_files = semantic_models_df['semantic_model_file'].dropna(
    ).tolist()

    if not semantic_model_files:
        st.error("No semantic model files found in agents.")
        st.stop()

    selected_model = st.selectbox(
        "Semantic Model",
        options=semantic_model_files,
        help="Select the semantic model to analyze"
    )

    st.divider()

    with st.spinner("Fetching Cortex Analyst data..."):
        analyst_df = get_cortex_analyst_latency_data(
            session, selected_model, start_date, end_date)

else:
    st.info("üìã Using SNOWFLAKE.LOCAL.CORTEX_ANALYST_REQUESTS_V view - requires ACCOUNTADMIN or CORTEX_ANALYST_REQUESTS_ADMIN role")

    st.divider()

    with st.spinner("Fetching all Cortex Analyst data (may take longer)..."):
        try:
            analyst_df = get_all_cortex_analyst_requests(
                session, start_date, end_date)
        except Exception as e:
            st.error(
                f"‚ùå Error accessing CORTEX_ANALYST_REQUESTS_V view: {str(e)}")
            st.error(
                "This view requires ACCOUNTADMIN or SNOWFLAKE.CORTEX_ANALYST_REQUESTS_ADMIN application role.")
            st.stop()

with st.spinner("Fetching SQL execution data..."):
    query_df = get_sql_execution_latency(session, start_date, end_date)

if len(analyst_df) == 0:
    st.warning("No Cortex Analyst requests found in the selected date range.")
    st.stop()

st.success(
    f"Found {len(analyst_df)} Cortex Analyst requests and {len(query_df)} executed queries")

analyst_df['ANALYST_LATENCY_SEC'] = analyst_df['ANALYST_LATENCY_MS'] / 1000
query_df['EXECUTION_LATENCY_SEC'] = query_df['EXECUTION_TIME_MS'] / 1000

tab1, tab2, tab3 = st.tabs(["üìä Cortex Analyst Analysis",
                           "‚ö° SQL Execution Analysis", "üîó Combined Analysis (Experimental)"])

with tab1:
    st.markdown("## üìä Cortex Analyst SQL Generation Latency")
    st.info("This section analyzes only the Cortex Analyst SQL generation performance (from CORTEX_ANALYST_REQUESTS)")

    if data_source == "All Semantic Models (ACCOUNTADMIN required)":
        st.markdown("### Breakdown by Semantic Model")
        model_breakdown = analyst_df.groupby('SEMANTIC_MODEL_NAME').agg({
            'REQUEST_ID': 'count',
            'ANALYST_LATENCY_SEC': ['mean', 'median', 'max']
        }).round(2)
        model_breakdown.columns = [
            'Request Count', 'Avg Latency (s)', 'Median Latency (s)', 'Max Latency (s)']
        model_breakdown = model_breakdown.sort_values(
            'Avg Latency (s)', ascending=False).reset_index()

        st.dataframe(model_breakdown,
                     use_container_width=True, hide_index=True)

        st.markdown("### Filter by Semantic Model")
        unique_models = [
            'All'] + analyst_df['SEMANTIC_MODEL_NAME'].dropna().unique().tolist()
        selected_filter_model = st.selectbox(
            "Filter results by semantic model",
            options=unique_models,
            key="model_filter"
        )

        if selected_filter_model != 'All':
            analyst_df_filtered = analyst_df[analyst_df['SEMANTIC_MODEL_NAME']
                                             == selected_filter_model]
        else:
            analyst_df_filtered = analyst_df
    else:
        analyst_df_filtered = analyst_df

    st.divider()

    col1, col2, col3 = st.columns(3)

    avg_analyst = analyst_df_filtered['ANALYST_LATENCY_SEC'].mean()
    median_analyst = analyst_df_filtered['ANALYST_LATENCY_SEC'].median()
    max_analyst = analyst_df_filtered['ANALYST_LATENCY_SEC'].max()

    col1.metric("Avg SQL Generation", f"{avg_analyst:.2f}s")
    col2.metric("Median SQL Generation", f"{median_analyst:.2f}s")
    col3.metric("Max SQL Generation", f"{max_analyst:.2f}s")

    st.markdown("### SQL Generation Latency Over Time")
    chart_df = analyst_df_filtered[[
        'TIMESTAMP', 'ANALYST_LATENCY_SEC']].sort_values('TIMESTAMP')
    st.line_chart(chart_df.set_index('TIMESTAMP')['ANALYST_LATENCY_SEC'])

    st.markdown("### Slowest Cortex Analyst Requests")
    top_n_analyst = st.slider("Number of slowest Cortex Analyst requests",
                              min_value=5, max_value=50, value=10, key="analyst_slider")

    slowest_analyst = analyst_df_filtered.nlargest(top_n_analyst, 'ANALYST_LATENCY_SEC')[[
        'TIMESTAMP',
        'REQUEST_ID',
        'USER_QUESTION',
        'GENERATED_SQL',
        'ANALYST_LATENCY_SEC',
        'USER_NAME',
        'SEMANTIC_MODEL_NAME'
    ]]

    st.dataframe(
        slowest_analyst,
        use_container_width=True,
        hide_index=True,
        column_config={
            'TIMESTAMP': st.column_config.DatetimeColumn('Time', format="MM/DD/YY HH:mm:ss"),
            'REQUEST_ID': 'Request ID',
            'USER_QUESTION': st.column_config.TextColumn('Question', width="medium"),
            'GENERATED_SQL': st.column_config.TextColumn('Generated SQL', width="large"),
            'ANALYST_LATENCY_SEC': st.column_config.NumberColumn('Latency (s)', format="%.2f"),
            'USER_NAME': 'User',
            'SEMANTIC_MODEL_NAME': 'Semantic Model'
        }
    )

with tab2:
    st.markdown("## ‚ö° SQL Execution Latency")
    st.info("This section analyzes SQL execution performance for queries filtered by '-- Generated by Cortex Analyst' comment")

    if len(query_df) == 0:
        st.warning(
            "No SQL execution queries found matching Cortex Analyst filter.")
    else:
        col1, col2 = st.columns(2)

        with col1:
            unique_query_tags = query_df['QUERY_TAG'].dropna(
            ).unique().tolist()
            query_tag_filter = st.multiselect(
                "Filter by Query Tag (Application)",
                options=['All'] + unique_query_tags,
                default=['All'],
                help="Filter queries by query_tag to analyze specific applications"
            )

        with col2:
            unique_warehouses = query_df['WAREHOUSE_NAME'].dropna(
            ).unique().tolist()
            warehouse_filter = st.multiselect(
                "Filter by Warehouse",
                options=['All'] + unique_warehouses,
                default=['All'],
                help="Filter queries by warehouse"
            )

        if 'All' not in query_tag_filter and len(query_tag_filter) > 0:
            filtered_query_df = query_df[query_df['QUERY_TAG'].isin(
                query_tag_filter)]
        else:
            filtered_query_df = query_df

        if 'All' not in warehouse_filter and len(warehouse_filter) > 0:
            filtered_query_df = filtered_query_df[filtered_query_df['WAREHOUSE_NAME'].isin(
                warehouse_filter)]

        col1, col2, col3 = st.columns(3)

        avg_exec = filtered_query_df['EXECUTION_LATENCY_SEC'].mean()
        median_exec = filtered_query_df['EXECUTION_LATENCY_SEC'].median()
        max_exec = filtered_query_df['EXECUTION_LATENCY_SEC'].max()

        col1.metric("Avg SQL Execution", f"{avg_exec:.2f}s")
        col2.metric("Median SQL Execution", f"{median_exec:.2f}s")
        col3.metric("Max SQL Execution", f"{max_exec:.2f}s")

        st.markdown("### SQL Execution Latency Over Time")
        exec_chart_df = filtered_query_df[[
            'START_TIME', 'EXECUTION_LATENCY_SEC']].sort_values('START_TIME')
        st.line_chart(exec_chart_df.set_index(
            'START_TIME')['EXECUTION_LATENCY_SEC'])

        st.markdown("### Query Tag Distribution")
        tag_counts = filtered_query_df['QUERY_TAG'].value_counts(
        ).reset_index()
        tag_counts.columns = ['Query Tag', 'Count']
        col1, col2 = st.columns([1, 2])
        with col1:
            st.dataframe(tag_counts, use_container_width=True, hide_index=True)
        with col2:
            st.bar_chart(tag_counts.set_index('Query Tag')['Count'])

        st.markdown("### Warehouse Distribution")
        warehouse_counts = filtered_query_df.groupby('WAREHOUSE_NAME').agg({
            'QUERY_ID': 'count',
            'EXECUTION_LATENCY_SEC': ['mean', 'median', 'max']
        }).round(2)
        warehouse_counts.columns = [
            'Query Count', 'Avg Latency (s)', 'Median Latency (s)', 'Max Latency (s)']
        warehouse_counts = warehouse_counts.sort_values(
            'Query Count', ascending=False).reset_index()
        col1, col2 = st.columns([1, 2])
        with col1:
            st.dataframe(warehouse_counts,
                         use_container_width=True, hide_index=True)
        with col2:
            st.bar_chart(warehouse_counts.set_index(
                'WAREHOUSE_NAME')['Query Count'])

        st.markdown("### Slowest SQL Executions")
        top_n_sql = st.slider("Number of slowest SQL executions",
                              min_value=5, max_value=50, value=10, key="sql_slider")

        slowest_sql = filtered_query_df.nlargest(top_n_sql, 'EXECUTION_LATENCY_SEC')[[
            'START_TIME',
            'QUERY_ID',
            'QUERY_TAG',
            'QUERY_TEXT',
            'EXECUTION_LATENCY_SEC',
            'USER_NAME',
            'WAREHOUSE_NAME'
        ]]

        st.dataframe(
            slowest_sql,
            use_container_width=True,
            hide_index=True,
            column_config={
                'START_TIME': st.column_config.DatetimeColumn('Time', format="MM/DD/YY HH:mm:ss"),
                'QUERY_ID': 'Query ID',
                'QUERY_TAG': 'Query Tag',
                'QUERY_TEXT': st.column_config.TextColumn('SQL Query', width="large"),
                'EXECUTION_LATENCY_SEC': st.column_config.NumberColumn('Execution (s)', format="%.2f"),
                'USER_NAME': 'User',
                'WAREHOUSE_NAME': 'Warehouse'
            }
        )

        with st.expander("View All SQL Execution Data"):
            st.dataframe(
                filtered_query_df[[
                    'START_TIME',
                    'QUERY_ID',
                    'QUERY_TAG',
                    'QUERY_TEXT',
                    'EXECUTION_LATENCY_SEC',
                    'USER_NAME',
                    'WAREHOUSE_NAME',
                    'EXECUTION_STATUS'
                ]],
                use_container_width=True,
                hide_index=True
            )

with tab3:
    st.markdown("## üîó Combined Analysis (Experimental)")
    st.warning("‚ö†Ô∏è Note: Joining Cortex Analyst logs with Query History by SQL text matching can be inaccurate. Use tabs 1 and 2 for more reliable analysis.")

    merged_df = join_latency_data(analyst_df, query_df)

    matches_found = merged_df['QUERY_ID'].notna().sum()
    st.info(
        f"Successfully matched {matches_found} out of {len(analyst_df)} Cortex Analyst requests to SQL executions")

    col1, col2, col3 = st.columns(3)

    avg_analyst_merged = merged_df['ANALYST_LATENCY_SEC'].mean()
    avg_execution_merged = merged_df['EXECUTION_LATENCY_SEC'].mean()
    avg_total = merged_df['TOTAL_LATENCY_SEC'].mean()

    col1.metric("Avg SQL Generation", f"{avg_analyst_merged:.2f}s")
    col2.metric("Avg SQL Execution", f"{avg_execution_merged:.2f}s")
    col3.metric("Avg Total Latency", f"{avg_total:.2f}s")

    st.markdown("### Latency Breakdown: SQL Generation vs Execution")

    matched_df = merged_df[merged_df['QUERY_ID'].notna()].copy()

    if len(matched_df) > 0:
        total_generation_time = matched_df['ANALYST_LATENCY_SEC'].sum()
        total_execution_time = matched_df['EXECUTION_LATENCY_SEC'].sum()
        total_combined_time = total_generation_time + total_execution_time

        gen_percentage = (total_generation_time / total_combined_time *
                          100) if total_combined_time > 0 else 0
        exec_percentage = (total_execution_time / total_combined_time *
                           100) if total_combined_time > 0 else 0

        col1, col2 = st.columns([1, 2])

        with col1:
            st.markdown("#### Latency Composition")
            breakdown_data = pd.DataFrame({
                'Component': ['SQL Generation', 'SQL Execution'],
                'Percentage': [gen_percentage, exec_percentage],
                'Total Time (s)': [total_generation_time, total_execution_time]
            })
            st.dataframe(
                breakdown_data,
                use_container_width=True,
                hide_index=True,
                column_config={
                    'Percentage': st.column_config.NumberColumn('% of Total', format="%.1f%%"),
                    'Total Time (s)': st.column_config.NumberColumn('Total Time (s)', format="%.2f")
                }
            )

            st.metric("SQL Generation", f"{gen_percentage:.1f}%", delta=None)
            st.metric("SQL Execution", f"{exec_percentage:.1f}%", delta=None)

        with col2:
            st.markdown("#### Visual Breakdown")
            import plotly.graph_objects as go

            fig = go.Figure(data=[go.Pie(
                labels=['SQL Generation (Cortex Analyst)',
                        'SQL Execution (Warehouse)'],
                values=[total_generation_time, total_execution_time],
                hole=.3,
                marker=dict(colors=['#29B5E8', '#FF6B6B']),
                textinfo='label+percent',
                textposition='auto',
                hovertemplate='<b>%{label}</b><br>Time: %{value:.2f}s<br>Percentage: %{percent}<extra></extra>'
            )])

            fig.update_layout(
                title_text="Total Latency Breakdown",
                showlegend=True,
                height=400,
                annotations=[dict(
                    text=f'{total_combined_time:.1f}s<br>Total', x=0.5, y=0.5, font_size=16, showarrow=False)]
            )

            st.plotly_chart(fig, use_container_width=True)
    else:
        st.warning("No matched queries to analyze latency breakdown.")

    st.markdown("### Combined Latency Distribution")
    col1, col2 = st.columns(2)

    with col1:
        st.markdown("##### SQL Generation Latency")
        latency_data = merged_df[['ANALYST_LATENCY_SEC']].rename(
            columns={'ANALYST_LATENCY_SEC': 'Seconds'})
        st.line_chart(latency_data)

    with col2:
        st.markdown("##### SQL Execution Latency")
        exec_data = merged_df[['EXECUTION_LATENCY_SEC']].rename(
            columns={'EXECUTION_LATENCY_SEC': 'Seconds'})
        st.line_chart(exec_data)

    st.markdown("### Slowest Combined Queries")
    top_n_combined = st.slider("Number of slowest queries",
                               min_value=5, max_value=50, value=10, key="combined_slider")

    slowest_combined = merged_df.nlargest(top_n_combined, 'TOTAL_LATENCY_SEC')[[
        'REQUEST_ID',
        'QUERY_ID',
        'USER_QUESTION',
        'GENERATED_SQL',
        'ANALYST_LATENCY_SEC',
        'EXECUTION_LATENCY_SEC',
        'TOTAL_LATENCY_SEC',
        'WAREHOUSE_NAME',
        'SEMANTIC_MODEL_NAME'
    ]]

    st.dataframe(
        slowest_combined,
        use_container_width=True,
        hide_index=True,
        column_config={
            'REQUEST_ID': 'Request ID',
            'QUERY_ID': 'Query ID',
            'USER_QUESTION': st.column_config.TextColumn('Question', width="medium"),
            'GENERATED_SQL': st.column_config.TextColumn('Generated SQL', width="large"),
            'ANALYST_LATENCY_SEC': st.column_config.NumberColumn('SQL Gen (s)', format="%.2f"),
            'EXECUTION_LATENCY_SEC': st.column_config.NumberColumn('Execution (s)', format="%.2f"),
            'TOTAL_LATENCY_SEC': st.column_config.NumberColumn('Total (s)', format="%.2f"),
            'WAREHOUSE_NAME': 'Warehouse',
            'SEMANTIC_MODEL_NAME': 'Semantic Model'
        }
    )

st.divider()

st.markdown("## üìÑ Export Report")

model_breakdown_for_report = None
if data_source == "All Semantic Models (ACCOUNTADMIN required)":
    model_breakdown_for_report = analyst_df.groupby('SEMANTIC_MODEL_NAME').agg({
        'REQUEST_ID': 'count',
        'ANALYST_LATENCY_SEC': ['mean', 'median', 'max']
    }).round(2)
    model_breakdown_for_report.columns = [
        'Request Count', 'Avg Latency (s)', 'Median Latency (s)', 'Max Latency (s)']
    model_breakdown_for_report = model_breakdown_for_report.sort_values(
        'Avg Latency (s)', ascending=False).reset_index()

query_tag_dist = None
warehouse_dist = None
slowest_sql_for_report = None

if len(query_df) > 0:
    query_tag_dist = query_df['QUERY_TAG'].value_counts().reset_index()
    query_tag_dist.columns = ['Query Tag', 'Count']

    warehouse_dist = query_df.groupby('WAREHOUSE_NAME').agg({
        'QUERY_ID': 'count',
        'EXECUTION_LATENCY_SEC': ['mean', 'median', 'max']
    }).round(2)
    warehouse_dist.columns = [
        'Query Count', 'Avg Latency (s)', 'Median Latency (s)', 'Max Latency (s)']
    warehouse_dist = warehouse_dist.sort_values(
        'Query Count', ascending=False).reset_index()

    slowest_sql_for_report = query_df.nlargest(10, 'EXECUTION_LATENCY_SEC')[[
        'START_TIME',
        'QUERY_ID',
        'QUERY_TAG',
        'EXECUTION_LATENCY_SEC',
        'USER_NAME',
        'WAREHOUSE_NAME'
    ]]

html_report = generate_html_report(
    analyst_df,
    slowest_analyst if 'slowest_analyst' in locals(
    ) else analyst_df.nlargest(10, 'ANALYST_LATENCY_SEC'),
    query_df=query_df if len(query_df) > 0 else None,
    slowest_sql=slowest_sql_for_report,
    model_breakdown=model_breakdown_for_report,
    query_tag_dist=query_tag_dist,
    warehouse_dist=warehouse_dist
)

st.download_button(
    label="üì• Download HTML Report",
    data=html_report,
    file_name=f"cortex_analyst_latency_report_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.html",
    mime="text/html"
)

st.caption("Snowflake Cortex Analyst Latency Report")
