{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Snowflake AI Cost Attribution Toolkit\n",
    "\n",
    "**Comprehensive cost analysis for Cortex Analyst and Cortex Agents**\n",
    "\n",
    "This notebook helps you:\n",
    "- üìä Track costs by semantic model and agent\n",
    "- üí∞ Combine Cortex feature credits with warehouse compute costs\n",
    "- üìà Analyze query patterns and performance\n",
    "- üéØ Identify optimization opportunities\n",
    "\n",
    "**Deployment**: Designed to run as a Snowflake Notebook in Snowsight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "\n",
    "session = get_active_session()\n",
    "print(\"‚úÖ Snowflake session initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "utils_header",
   "metadata": {},
   "source": [
    "## 2. Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "utils_functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_semantic_model_paths(session):\n",
    "    \"\"\"\n",
    "    Fetch semantic model paths from all agents in the schema.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with agent names, tool names, and semantic model file paths\n",
    "    \"\"\"\n",
    "    try:\n",
    "        agents_result = session.sql(\n",
    "            \"SHOW AGENTS IN SCHEMA snowflake_intelligence.agents\").collect()\n",
    "\n",
    "        if not agents_result:\n",
    "            print(\"‚ö†Ô∏è  No agents found in SNOWFLAKE_INTELLIGENCE.AGENTS schema\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        results = []\n",
    "        agent_names = [row[1] for row in agents_result]\n",
    "\n",
    "        for agent in agent_names:\n",
    "            try:\n",
    "                describe_sql = f'DESCRIBE AGENT SNOWFLAKE_INTELLIGENCE.AGENTS.\"{agent}\"'\n",
    "                describe_result = session.sql(describe_sql).collect()\n",
    "\n",
    "                if not describe_result:\n",
    "                    continue\n",
    "\n",
    "                agent_spec_json = describe_result[0][6] if len(\n",
    "                    describe_result[0]) > 6 else None\n",
    "\n",
    "                if agent_spec_json:\n",
    "                    try:\n",
    "                        spec = json.loads(agent_spec_json)\n",
    "\n",
    "                        if 'tool_resources' in spec:\n",
    "                            for tool_name, tool_data in spec['tool_resources'].items():\n",
    "                                semantic_file = tool_data.get('semantic_model_file')\n",
    "                                results.append({\n",
    "                                    \"agent_name\": agent,\n",
    "                                    \"tool_name\": tool_name,\n",
    "                                    \"semantic_model_file\": semantic_file\n",
    "                                })\n",
    "                    except json.JSONDecodeError:\n",
    "                        continue\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è  Could not describe agent {agent}: {e}\")\n",
    "                continue\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Could not access SNOWFLAKE_INTELLIGENCE.AGENTS schema: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "def get_cortex_analyst_logs_for_all_semantic_models(session):\n",
    "    \"\"\"\n",
    "    Retrieve Cortex Analyst logs for a specific semantic model file.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with Cortex Analyst logs including credits\n",
    "    \"\"\"\n",
    "\n",
    "    cortex_analyst_log = session.sql(\"\"\"SELECT \n",
    "          timestamp::STRING as timestamp,\n",
    "          request_id,\n",
    "          semantic_model_name,\n",
    "          tables_referenced,\n",
    "          user_name,\n",
    "          source,\n",
    "          feedback,\n",
    "          response_status_code,\n",
    "          request_body:messages[0].content[0].text::STRING as user_question,\n",
    "          response_body:response_metadata.analyst_latency_ms::NUMBER as latency_ms,\n",
    "          generated_sql,\n",
    "          response_body:response_metadata.analyst_orchestration_path::STRING as orchestration_path,\n",
    "          response_body:response_metadata.question_category::STRING as question_category,\n",
    "          response_body:message.content[1].confidence.verified_query_used.name::STRING as verified_query_name,\n",
    "          response_body:message.content[1].confidence.verified_query_used.question::STRING as verified_query_question\n",
    "        FROM SNOWFLAKE.LOCAL.CORTEX_ANALYST_REQUESTS_V\n",
    "        WHERE timestamp >= DATEADD(DAY, -30, CURRENT_TIMESTAMP())\"\"\")\n",
    "\n",
    "    df = cortex_analyst_log.to_pandas()\n",
    "    if not df.empty:\n",
    "        df[\"QUERY_TYPE\"] = df[\"ORCHESTRATION_PATH\"].apply(\n",
    "            lambda x: \"Verified Query\" if x == \"vqr_fast_path\" else \"Non-Verified Query\"\n",
    "        )\n",
    "        df['CORTEX_ANALYST_CREDITS'] = 67/1000\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def get_cortex_analyst_logs(session, semantic_model_file):\n",
    "    \"\"\"\n",
    "    Retrieve Cortex Analyst logs for a specific semantic model file.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with Cortex Analyst logs including credits\n",
    "    \"\"\"\n",
    "    cortex_analyst_log = session.sql(f'''SELECT \n",
    "          timestamp::STRING as timestamp,\n",
    "          request_id,\n",
    "          semantic_model_name,\n",
    "          tables_referenced,\n",
    "          user_name,\n",
    "          source,\n",
    "          feedback,\n",
    "          response_status_code,\n",
    "          request_body:messages[0].content[0].text::STRING as user_question,\n",
    "          response_body:response_metadata.analyst_latency_ms::NUMBER as latency_ms,\n",
    "          generated_sql,\n",
    "          response_body:response_metadata.analyst_orchestration_path::STRING as orchestration_path,\n",
    "          response_body:response_metadata.question_category::STRING as question_category,\n",
    "          response_body:message.content[1].confidence.verified_query_used.name::STRING as verified_query_name,\n",
    "         response_body:message.content[1].confidence.verified_query_used.question::STRING as verified_query_question\n",
    "        FROM TABLE(\n",
    "          SNOWFLAKE.LOCAL.CORTEX_ANALYST_REQUESTS('FILE_ON_STAGE', '{semantic_model_file}'))''')\n",
    "\n",
    "    df = cortex_analyst_log.to_pandas()\n",
    "    df[\"QUERY_TYPE\"] = df[\"ORCHESTRATION_PATH\"].apply(\n",
    "        lambda x: \"Verified Query\" if x == \"vqr_fast_path\" else \"Non-Verified Query\"\n",
    "    )\n",
    "    df['CORTEX_ANALYST_CREDITS'] = 67/1000\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_sf_intelligence_query_history(session, target_table=\"CORTEX_ANALYTICS.PUBLIC.SF_INTELLIGENCE_QUERY_HISTORY\", force_refresh=False):\n",
    "    \"\"\"\n",
    "    Create or refresh query history table with compute credits.\n",
    "    Captures queries from Cortex Agents and manually executed Cortex Analyst SQLs.\n",
    "    \n",
    "    Args:\n",
    "        force_refresh: If True, recreates the table with fresh data\n",
    "    \"\"\"\n",
    "    table_parts = target_table.split('.')\n",
    "    table_name = table_parts[-1].upper()\n",
    "    schema_name = table_parts[-2].upper() if len(table_parts) > 1 else 'PUBLIC'\n",
    "    database_name = table_parts[-3].upper() if len(table_parts) > 2 else 'CORTEX_ANALYTICS'\n",
    "\n",
    "    # Check if table exists\n",
    "    table_exists = False\n",
    "    try:\n",
    "        result = session.sql(f\"\"\"\n",
    "            SELECT COUNT(*) as table_count \n",
    "            FROM INFORMATION_SCHEMA.TABLES \n",
    "            WHERE TABLE_SCHEMA = '{schema_name}' \n",
    "            AND TABLE_NAME = '{table_name}' \n",
    "            AND TABLE_CATALOG = '{database_name}'\n",
    "        \"\"\").collect()\n",
    "        table_exists = result[0]['TABLE_COUNT'] > 0\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Could not check table: {e}\")\n",
    "\n",
    "    if table_exists and not force_refresh:\n",
    "        print(f\"‚úÖ Table {target_table} already exists (use force_refresh=True to recreate)\")\n",
    "        return\n",
    "\n",
    "    # Drop and recreate with fresh data\n",
    "    if table_exists:\n",
    "        print(f\"üîÑ Refreshing {target_table}...\")\n",
    "        session.sql(f\"DROP TABLE {target_table}\").collect()\n",
    "\n",
    "    query = f\"\"\"\n",
    "    CREATE TABLE {target_table} AS\n",
    "    SELECT \n",
    "        qh.query_id,\n",
    "        qh.query_text,\n",
    "        qh.query_tag,\n",
    "        qh.start_time,\n",
    "        qh.total_elapsed_time,\n",
    "        qh.warehouse_name,\n",
    "        qh.user_name,\n",
    "        COALESCE(qah.credits_attributed_compute, 0) as credits_attributed_compute,\n",
    "        TRIM(REGEXP_REPLACE(\n",
    "            REGEXP_REPLACE(\n",
    "                REGEXP_REPLACE(qh.query_text, '--[^\\\\n]*\\\\n', '\\\\n'),\n",
    "                '/\\\\*.*?\\\\*/', ' '\n",
    "            ),\n",
    "            '\\\\s+', ' '\n",
    "        )) AS cleaned_query_text\n",
    "    FROM SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY qh\n",
    "    LEFT JOIN SNOWFLAKE.ACCOUNT_USAGE.QUERY_ATTRIBUTION_HISTORY qah \n",
    "        ON qh.query_id = qah.query_id\n",
    "    WHERE qh.start_time >= DATEADD(DAY, -30, CURRENT_TIMESTAMP())\n",
    "      AND (\n",
    "          qh.query_tag = 'cortex-agent'\n",
    "          OR qh.query_text ILIKE '%__media_mix_modeling_agg%'\n",
    "          OR qh.query_text ILIKE '%__co_branding_agreements%'\n",
    "          OR qh.query_text ILIKE '%__sp500%'\n",
    "      )\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        session.sql(query).collect()\n",
    "        \n",
    "        # Check how many records we got\n",
    "        count_result = session.sql(f\"SELECT COUNT(*) as cnt FROM {target_table}\").collect()\n",
    "        record_count = count_result[0]['CNT']\n",
    "        \n",
    "        print(f\"‚úÖ Table {target_table} created with {record_count:,} records\")\n",
    "        \n",
    "        if record_count == 0:\n",
    "            print(\"‚ö†Ô∏è  Warning: No matching queries found in last 30 days\")\n",
    "            print(\"   This could mean:\")\n",
    "            print(\"   - No Cortex Analyst queries were executed via agents\")\n",
    "            print(\"   - Queries don't have 'cortex-agent' tag\")\n",
    "            print(\"   - ACCOUNT_USAGE.QUERY_HISTORY has a latency (up to 45 min)\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creating table: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def normalize_sql(sql_text):\n",
    "    \"\"\"Normalize SQL text for better matching between logs and query history.\"\"\"\n",
    "    if pd.isna(sql_text) or sql_text is None:\n",
    "        return \"\"\n",
    "\n",
    "    sql_str = str(sql_text).strip()\n",
    "\n",
    "    sql_str = re.sub(r'doc_ai_q\\s*_db\\.doc_ai_\\s*chema',\n",
    "                     'doc_ai_qs_db.doc_ai_schema', sql_str)\n",
    "    sql_str = re.sub(r'co_branding_agreement\\s+',\n",
    "                     'co_branding_agreements ', sql_str)\n",
    "    sql_str = re.sub(r'__co_branding_agreement\\b',\n",
    "                     '__co_branding_agreements', sql_str)\n",
    "    sql_str = re.sub(r'have_renewal_option\\s*_value',\n",
    "                     'have_renewal_options_value', sql_str)\n",
    "    sql_str = re.sub(r'have_force_majeure\\s*_value',\n",
    "                     'have_force_majeure_value', sql_str)\n",
    "\n",
    "    sql_normalized = re.sub(r'\\s+', ' ', sql_str.upper())\n",
    "    sql_normalized = sql_normalized.rstrip(';')\n",
    "    sql_normalized = re.sub(r'\\bAS\\s+(\\w+)', r'AS \\1', sql_normalized)\n",
    "\n",
    "    return sql_normalized\n",
    "\n",
    "\n",
    "def create_cortex_analyst_query_history(session, cortex_analyst_df, sf_intelligence_table=\"CORTEX_ANALYTICS.PUBLIC.SF_INTELLIGENCE_QUERY_HISTORY\"):\n",
    "    \"\"\"\n",
    "    Join Cortex Analyst logs with query history to get full cost attribution.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with combined Cortex credits and warehouse costs\n",
    "    \"\"\"\n",
    "    sql = f\"SELECT * FROM {sf_intelligence_table}\"\n",
    "    pd_sfi_query_history = session.sql(sql).to_pandas()\n",
    "\n",
    "    pd_sfi_query_history_copy = pd_sfi_query_history.copy()\n",
    "    cortex_analyst_copy = cortex_analyst_df.copy()\n",
    "\n",
    "    pd_sfi_query_history_copy['NORMALIZED_SQL'] = pd_sfi_query_history_copy['CLEANED_QUERY_TEXT'].apply(\n",
    "        normalize_sql)\n",
    "    cortex_analyst_copy['NORMALIZED_SQL'] = cortex_analyst_copy['GENERATED_SQL'].apply(\n",
    "        normalize_sql)\n",
    "\n",
    "    pd_sfi_query_history_copy = pd_sfi_query_history_copy[\n",
    "        pd_sfi_query_history_copy['NORMALIZED_SQL'] != \"\"]\n",
    "    cortex_analyst_copy = cortex_analyst_copy[cortex_analyst_copy['NORMALIZED_SQL'] != \"\"]\n",
    "\n",
    "    pd_sfi_query_history_indexed = pd_sfi_query_history_copy.set_index(\n",
    "        \"NORMALIZED_SQL\")\n",
    "    cortex_analyst_indexed = cortex_analyst_copy.set_index(\"NORMALIZED_SQL\")\n",
    "\n",
    "    ca_query_history = pd.merge(\n",
    "        pd_sfi_query_history_indexed,\n",
    "        cortex_analyst_indexed,\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "        how='inner'\n",
    "    )\n",
    "\n",
    "    if len(ca_query_history) == 0:\n",
    "        pd_sfi_query_history_copy['SQL_FRAGMENT'] = pd_sfi_query_history_copy['NORMALIZED_SQL'].apply(\n",
    "            lambda x: x[:100] if x else \"\")\n",
    "        cortex_analyst_copy['SQL_FRAGMENT'] = cortex_analyst_copy['NORMALIZED_SQL'].apply(\n",
    "            lambda x: x[:100] if x else \"\")\n",
    "\n",
    "        pd_sfi_fuzzy = pd_sfi_query_history_copy[pd_sfi_query_history_copy['SQL_FRAGMENT'] != \"\"].set_index(\n",
    "            'SQL_FRAGMENT')\n",
    "        cortex_fuzzy = cortex_analyst_copy[cortex_analyst_copy['SQL_FRAGMENT'] != \"\"].set_index(\n",
    "            'SQL_FRAGMENT')\n",
    "\n",
    "        ca_query_history = pd.merge(\n",
    "            pd_sfi_fuzzy,\n",
    "            cortex_fuzzy,\n",
    "            left_index=True,\n",
    "            right_index=True,\n",
    "            how='inner'\n",
    "        )\n",
    "\n",
    "    ca_query_history['TOTAL_TIME'] = (\n",
    "        ca_query_history[\"TOTAL_ELAPSED_TIME\"] + ca_query_history[\"LATENCY_MS\"]\n",
    "    )\n",
    "\n",
    "    ca_query_history['TOTAL_CREDITS_WH_AND_CA'] = (\n",
    "        ca_query_history[\"CREDITS_ATTRIBUTED_COMPUTE\"] +\n",
    "        ca_query_history[\"CORTEX_ANALYST_CREDITS\"]\n",
    "    )\n",
    "\n",
    "    ca_query_history = ca_query_history.reset_index()\n",
    "\n",
    "    return ca_query_history\n",
    "\n",
    "\n",
    "def total_cost_by_semantic_model(ca_query_history_df):\n",
    "    \"\"\"\n",
    "    Calculate total cost (Cortex + Warehouse) by semantic model.\n",
    "    \"\"\"\n",
    "    total_cost_by_model = (\n",
    "        ca_query_history_df[['SEMANTIC_MODEL_NAME', 'TOTAL_CREDITS_WH_AND_CA']]\n",
    "        .groupby(['SEMANTIC_MODEL_NAME'])\n",
    "        .sum()\n",
    "        .round(4)\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    total_cost_by_model = total_cost_by_model.sort_values(\n",
    "        'TOTAL_CREDITS_WH_AND_CA',\n",
    "        ascending=False\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    return total_cost_by_model\n",
    "\n",
    "\n",
    "def cost_breakdown_by_semantic_model(ca_query_history_df):\n",
    "    \"\"\"\n",
    "    Detailed cost breakdown showing separate Cortex and warehouse costs.\n",
    "    \"\"\"\n",
    "    cost_breakdown = ca_query_history_df.groupby('SEMANTIC_MODEL_NAME').agg({\n",
    "        'CORTEX_ANALYST_CREDITS': 'sum',\n",
    "        'CREDITS_ATTRIBUTED_COMPUTE': 'sum',\n",
    "        'TOTAL_CREDITS_WH_AND_CA': 'sum',\n",
    "        'REQUEST_ID': 'count'\n",
    "    }).rename(columns={\n",
    "        'CORTEX_ANALYST_CREDITS': 'cortex_analyst_credits',\n",
    "        'CREDITS_ATTRIBUTED_COMPUTE': 'warehouse_credits',\n",
    "        'TOTAL_CREDITS_WH_AND_CA': 'total_credits',\n",
    "        'REQUEST_ID': 'query_count'\n",
    "    }).round(4).reset_index()\n",
    "\n",
    "    total_all_credits = cost_breakdown['total_credits'].sum()\n",
    "    cost_breakdown['percentage_of_total_cost'] = round(\n",
    "        (cost_breakdown['total_credits'] / total_all_credits) * 100, 2\n",
    "    )\n",
    "\n",
    "    cost_breakdown['avg_credits_per_query'] = round(\n",
    "        cost_breakdown['total_credits'] / cost_breakdown['query_count'], 4\n",
    "    )\n",
    "\n",
    "    cost_breakdown = cost_breakdown.sort_values(\n",
    "        'total_credits',\n",
    "        ascending=False\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    return cost_breakdown\n",
    "\n",
    "\n",
    "def cost_by_agent(semantic_model_paths_df, cost_by_model_df):\n",
    "    \"\"\"\n",
    "    Aggregate costs by agent (multiple semantic models per agent).\n",
    "    \"\"\"\n",
    "    merged = semantic_model_paths_df.merge(\n",
    "        cost_by_model_df,\n",
    "        left_on='semantic_model_file',\n",
    "        right_on='SEMANTIC_MODEL_NAME',\n",
    "        how='inner'\n",
    "    )\n",
    "\n",
    "    agent_costs = merged.groupby('agent_name').agg({\n",
    "        'TOTAL_CREDITS_WH_AND_CA': 'sum',\n",
    "        'tool_name': 'count'\n",
    "    }).rename(columns={\n",
    "        'TOTAL_CREDITS_WH_AND_CA': 'total_credits',\n",
    "        'tool_name': 'tool_count'\n",
    "    }).round(4).reset_index()\n",
    "\n",
    "    agent_costs = agent_costs.sort_values(\n",
    "        'total_credits',\n",
    "        ascending=False\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    return agent_costs\n",
    "\n",
    "\n",
    "print(\"‚úÖ Utility functions loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "discover",
   "metadata": {},
   "source": [
    "## 3. Discover Semantic Models and Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fetch_models",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç Discovering semantic models and agents...\\n\")\n",
    "\n",
    "df_semantic_models = fetch_semantic_model_paths(session)\n",
    "\n",
    "if not df_semantic_models.empty:\n",
    "    print(f\"‚úÖ Found {len(df_semantic_models)} semantic model configurations\\n\")\n",
    "    display(df_semantic_models)\n",
    "    \n",
    "    unique_agents = df_semantic_models['agent_name'].nunique()\n",
    "    unique_models = df_semantic_models['semantic_model_file'].nunique()\n",
    "    print(f\"\\nüìä Summary: {unique_agents} agents, {unique_models} unique semantic models\")\n",
    "    print(f\"\\n‚ÑπÔ∏è  Note: All Cortex Analyst logs will be loaded from CORTEX_ANALYST_REQUESTS_V\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No agents found - will still load all Cortex Analyst logs\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup_tables",
   "metadata": {},
   "source": [
    "## 4. Setup Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create_tables",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìã Setting up cost tracking tables...\\n\")\n",
    "\n",
    "database_name = session.get_current_database()\n",
    "schema_name = session.get_current_schema()\n",
    "target_table = f\"{database_name}.{schema_name}.CORTEX_ANALYST_LOGS\"\n",
    "query_history_table = f\"{database_name}.{schema_name}.SF_INTELLIGENCE_QUERY_HISTORY\"\n",
    "\n",
    "session.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {target_table} (\n",
    "    TIMESTAMP                STRING,\n",
    "    REQUEST_ID               STRING,\n",
    "    SEMANTIC_MODEL_NAME      STRING,\n",
    "    TABLES_REFERENCED        STRING,\n",
    "    USER_NAME                STRING,\n",
    "    SOURCE                   STRING,\n",
    "    FEEDBACK                 STRING,\n",
    "    RESPONSE_STATUS_CODE     INTEGER,\n",
    "    USER_QUESTION            STRING,\n",
    "    LATENCY_MS               NUMBER,\n",
    "    GENERATED_SQL            STRING,\n",
    "    ORCHESTRATION_PATH       STRING,\n",
    "    QUESTION_CATEGORY        STRING,\n",
    "    VERIFIED_QUERY_NAME      STRING,\n",
    "    VERIFIED_QUERY_QUESTION  STRING,\n",
    "    QUERY_TYPE               STRING,\n",
    "    CORTEX_ANALYST_CREDITS   FLOAT\n",
    ")\n",
    "\"\"\").collect()\n",
    "print(f\"‚úÖ Created/verified {target_table}\")\n",
    "\n",
    "create_sf_intelligence_query_history(session, query_history_table, force_refresh=True)\n",
    "\n",
    "print(\"\\n‚úÖ All tables ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "populate",
   "metadata": {},
   "source": [
    "## 5. Populate Cortex Analyst Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_logs",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä Populating Cortex Analyst logs...\\n\")\n",
    "\n",
    "session.sql(f\"TRUNCATE TABLE {target_table}\").collect()\n",
    "print(\"üóëÔ∏è  Cleared existing logs\\n\")\n",
    "\n",
    "total_logs = 0\n",
    "\n",
    "if not df_semantic_models.empty:\n",
    "    try:\n",
    "        df = get_cortex_analyst_logs_for_all_semantic_models(session)\n",
    "\n",
    "        if not df.empty:\n",
    "            session.write_pandas(\n",
    "                df,\n",
    "                table_name=target_table.split('.')[-1],\n",
    "                database=database_name,\n",
    "                schema=schema_name,\n",
    "                auto_create_table=False,\n",
    "                overwrite=False\n",
    "            )\n",
    "            total_logs = len(df)\n",
    "            print(f'Found {total_logs} logs')\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error processing : {e}\")\n",
    "\n",
    "        print(f\"\\n‚úÖ Loaded {total_logs:,} total log entries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cost_analysis",
   "metadata": {},
   "source": [
    "## 6. Cost Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyze_costs",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üí∞ Analyzing costs...\\n\")\n",
    "\n",
    "cortex_logs_df = session.sql(f\"SELECT * FROM {target_table}\").to_pandas()\n",
    "\n",
    "if not cortex_logs_df.empty:\n",
    "    print(f\"üìä Retrieved {len(cortex_logs_df)} Cortex Analyst log records\\n\")\n",
    "    \n",
    "    query_history_count = session.sql(f\"SELECT COUNT(*) as count FROM {query_history_table}\").collect()[0]['COUNT']\n",
    "    print(f\"üìã Found {query_history_count:,} query history records\\n\")\n",
    "    \n",
    "    if query_history_count > 0:\n",
    "        print(\"üîó Joining Cortex logs with query history...\")\n",
    "        ca_query_history = create_cortex_analyst_query_history(\n",
    "            session, \n",
    "            cortex_logs_df, \n",
    "            query_history_table\n",
    "        )\n",
    "        \n",
    "        if not ca_query_history.empty:\n",
    "            print(f\"‚úÖ Matched {len(ca_query_history)} records ({len(ca_query_history)/len(cortex_logs_df)*100:.1f}% match rate)\\n\")\n",
    "            \n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"üí∏ TOTAL COST BY SEMANTIC MODEL\")\n",
    "            print(\"=\"*60)\n",
    "            total_costs = total_cost_by_semantic_model(ca_query_history)\n",
    "            display(total_costs)\n",
    "            \n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"üìä DETAILED COST BREAKDOWN\")\n",
    "            print(\"=\"*60)\n",
    "            cost_breakdown = cost_breakdown_by_semantic_model(ca_query_history)\n",
    "            display(cost_breakdown)\n",
    "            \n",
    "            if not df_semantic_models.empty:\n",
    "                print(\"\\n\" + \"=\"*60)\n",
    "                print(\"ü§ñ COST BY AGENT\")\n",
    "                print(\"=\"*60)\n",
    "                agent_costs = cost_by_agent(df_semantic_models, total_costs)\n",
    "                display(agent_costs)\n",
    "                \n",
    "                total_agent_cost = agent_costs['total_credits'].sum()\n",
    "                print(f\"\\nüí∞ Total cost across all agents: {total_agent_cost:.4f} credits\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è  No matching records found between Cortex logs and query history\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  No query history data available for cost analysis\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No Cortex Analyst logs found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## 7. Summary\n",
    "\n",
    "### üìä What This Notebook Provides:\n",
    "\n",
    "1. **Cost by Semantic Model**: Detailed attribution for each semantic model file\n",
    "2. **Cost by Agent**: Aggregated costs across all tools within an agent\n",
    "3. **Full Cost Visibility**: Combines:\n",
    "   - Cortex Analyst feature credits (text-to-SQL)\n",
    "   - Warehouse compute credits (SQL execution)\n",
    "\n",
    "### üéØ Use Cases:\n",
    "\n",
    "- Track which agents/models are most expensive\n",
    "- Optimize high-cost semantic models\n",
    "- Budget planning and forecasting\n",
    "- Chargeback to business units\n",
    "\n",
    "### üîÑ Refresh Data:\n",
    "\n",
    "Run cells 5-6 to refresh logs and recompute costs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}